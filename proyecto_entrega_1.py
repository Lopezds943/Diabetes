# -*- coding: utf-8 -*-
"""Proyecto Entrega 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KuGCEkQzUpyxk6nJE2k5TqbylEbhblw3

##**Diabetes 130-Hospitales de EE. UU**

El conjunto de datos representa diez años (1999-2008) de atención clínica en 130 hospitales de EE. UU. y redes de prestación integradas.

Los datos contienen atributos como el número de pacientes, la raza, el sexo, la edad, el tipo de ingreso, el tiempo en el hospital, la especialidad médica del médico de admisión, el número de pruebas de laboratorio realizadas, el resultado de la prueba de HbA1c, el diagnóstico, el número de medicamentos, los medicamentos para la diabetes, el número de visitas ambulatorias, hospitalarias y de emergencia en el año anterior a la hospitalización, etc.

---
---
# **1. Obtener los datos**

---
---
"""

# example of chi squared feature selection for categorical data
from pandas import read_csv
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from matplotlib import pyplot

#Montar Googgle Drive y mover los archivos ahí
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv("/content/drive/MyDrive/ML-BIOESTADISTICA/diabetic_data.csv")
df.head()

# Ver valores únicos en las columnas categóricas codificadas
print("admission_type_id:", sorted(df["admission_type_id"].unique()))
print("discharge_disposition_id:", sorted(df["discharge_disposition_id"].unique()))
print("admission_source_id:", sorted(df["admission_source_id"].unique()))

#Convertir a string
df["admission_type_id"] = df["admission_type_id"].astype(str)
df["discharge_disposition_id"] = df["discharge_disposition_id"].astype(str)
df["admission_source_id"] = df["admission_source_id"].astype(str)

map_admission_type = {
    "1": "Emergencia",
    "2": "Urgente",
    "3": "Electivo",
    "4": "Recién nacido",
    "5": "Sin_info",
    "6": "Sin_info",
    "7": "Centro de trauma",
    "8": "Sin_info"
}

map_discharge_disposition = {
    "1": "Alta a casa",
    "2": "Alta a otro hospital a corto plazo",
    "3": "Alta a centro de enfermería especializada",
    "4": "Alta a centro de cuidados intermedios",
    "5": "Alta a otro tipo de atención hospitalaria",
    "6": "Cuidados de salud en casa",
    "7": "Salida contra recomendación médica",
    "8": "Alta a casa con cuidados",
    "9": "Admitido como paciente hospitalizado",
    "10": "Cuidados paliativos en casa",
    "11": "Cuidados paliativos en centro médico",
    "12": "Alta a hospital psiquiátrico",
    "13": "Alta a otra instalación de rehabilitación",
    "14": "Sin_info",
    "15": "Sin_info",
    "16": "Alta a hospital federal",
    "17": "Alta a otra institución",
    "18": "Alta a custodia policial",
    "19": "Sin_info",
    "20": "Alta por orden judicial",
    "21": "Sin_info",
    "22": "Falleció en casa",
    "23": "Falleció en instalación médica",
    "24": "Falleció en lugar desconocido",
    "25": "Falleció en cuidados paliativos",
    "28": "Falleció en centro de enfermería especializada"
}

map_admission_source = {
    "1": "Referencia médica",
    "2": "Referencia desde clínica",
    "3": "Referencia desde aseguradora HMO",
    "4": "Transferencia desde hospital",
    "5": "Transferencia desde centro de enfermería especializada",
    "6": "Transferencia desde otro centro de salud",
    "7": "Sala de emergencias",
    "8": "Corte o custodia policial",
    "9": "Sin_info",
    "10": "Transferencia desde hospital de acceso crítico",
    "11": "Parto normal",
    "12": "Sin_info",
    "13": "Nacido en hospital",
    "14": "Nacido fuera de hospital",
    "15": "Sin_info",
    "17": "Sin_info",
    "20": "Sin_info",
    "22": "Sin_info",
    "25": "Sin_info"

}

# --- 3. Reemplazar con map() y 'Desconocido' para valores fuera del diccionario ---
df["admission_type_id"] = df["admission_type_id"].map(lambda x: map_admission_type.get(x, "Desconocido"))
df["discharge_disposition_id"] = df["discharge_disposition_id"].map(lambda x: map_discharge_disposition.get(x, "Desconocido"))
df["admission_source_id"] = df["admission_source_id"].map(lambda x: map_admission_source.get(x, "Desconocido"))

# --- 4. Verificación rápida ---
print(df[["admission_type_id", "discharge_disposition_id", "admission_source_id"]].head())

# Tamaño y forma del Dataset
print("Shape:", df.shape)
print("Número de filas:", df.shape[0])
print("Número de columnas:", df.shape[1])

# Mostrar información general del DataFrame
print("--- Información del DataFrame ---")
df.info()

# Revisar valores faltantes
df.isnull().sum()

faltantes = df.isnull().mean() * 100
print(faltantes.sort_values(ascending=False))

# Para columnas categóricas
for col in df.select_dtypes(include='object'):
    print(f"\nValores únicos en {col}:")
    print(df[col].value_counts())

# Revisar duplicados
print("Duplicados:", df.duplicated().sum())

# Ver las filas duplicadas
df[df.duplicated()]

missing_vals = ["None", "?"]
df = df.replace(missing_vals, pd.NA)

# Verificar resultado en algunas columnas
print(df['max_glu_serum'].value_counts(dropna=False))
print(df['weight'].value_counts(dropna=False))

faltantes = df.isna().sum().sort_values(ascending=False)
faltantes_pct = (df.isna().mean() * 100).sort_values(ascending=False)

faltantes_df = pd.DataFrame({
    'Faltantes': faltantes,
    'Porcentaje': faltantes_pct
})
print(faltantes_df)

cat_cols = df.select_dtypes(include='object').columns

for col in cat_cols:
    if df[col].isna().sum() > 0:
        df[col] = df[col].fillna("Sin_info")
print(df[cat_cols].isna().sum().sort_values(ascending=False))

df

df = df.drop(columns=["encounter_id", "patient_nbr"])

#Variables numéricas (PCA)
num_cols_pca = [
    "time_in_hospital", "num_lab_procedures", "num_procedures",
    "num_medications", "number_outpatient", "number_emergency",
    "number_inpatient", "number_diagnoses"
]

#Variables categóricas (MCA)
cat_cols_mca = [
    "race", "gender", "age", "weight", "payer_code",
    "medical_specialty", "diag_1", "diag_2", "diag_3",
    "max_glu_serum", "A1Cresult",
    "metformin", "repaglinide", "nateglinide", "chlorpropamide",
    "glimepiride", "acetohexamide", "glipizide", "glyburide",
    "tolbutamide", "pioglitazone", "rosiglitazone", "acarbose",
    "miglitol", "troglitazone", "tolazamide", "examide",
    "citoglipton", "insulin", "glyburide-metformin",
    "glipizide-metformin", "glimepiride-pioglitazone",
    "metformin-rosiglitazone", "metformin-pioglitazone",
    "change", "diabetesMed"
]

#Categóricas que estan con códigos numéricos
categoricas_codigos = [
    "admission_type_id", "discharge_disposition_id", "admission_source_id"
]

#  las tenemos en cuenta para al MCA
cat_cols_mca.extend(categoricas_codigos)

# Verificación
print("PCA (numéricas):", len(num_cols_pca), num_cols_pca)
print("MCA (categóricas):", len(cat_cols_mca), cat_cols_mca)

df["readmitted"].value_counts()

x= df.drop("readmitted", axis=1)
y= df["readmitted"]

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test  = train_test_split(x,y, test_size=0.2, stratify=y, random_state=42)

"""# TAREA 1: PCA + MCA"""

from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

#PCA
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[num_cols_pca])

# PCA con todos los componentes
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Varianza acumulada
explained_var = np.cumsum(pca.explained_variance_ratio_)

plt.figure(figsize=(8,5))
plt.plot(range(1, len(explained_var) + 1), explained_var, marker='o', linestyle='--')
plt.axhline(y=0.85, color='r', linestyle='-')
plt.xlabel('Número de componentes principales')
plt.ylabel('Varianza acumulada explicada')
plt.title('Varianza acumulada explicada por PCA')
plt.grid(True)
plt.show()

# Scatterplot PC1 vs PC2
plt.figure(figsize=(8,6))
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y, palette='Set1', alpha=0.7)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Scatterplot PC1 vs PC2')
plt.legend(title='Clase', labels=['Benigno', 'Maligno'])
plt.show()

# Loadings (cargas) de las variables en las PCs
loadings = pd.DataFrame(pca.components_.T,columns=[f'PC{i+1}' for i in range(pca.n_components_)],index=num_cols_pca)

plt.figure(figsize=(12,8))
sns.heatmap(loadings.iloc[:,:10], annot=True, cmap='coolwarm', center=0)
plt.title('Heatmap de loadings (primeras PCs)')
plt.show()

# Aplicar PCA
pca = PCA(n_components=0.85)
X_pca = pca.fit_transform(X_scaled)

print(f"Número de componentes principales para explicar 85% varianza: {pca.n_components_}")
print(f"Varianza explicada acumulada por estas componentes: {sum(pca.explained_variance_ratio_):.4f}")

!pip install mca

!pip install prince

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import mca
import prince
from sklearn.datasets import load_breast_cancer

X_cat = df[cat_cols_mca].astype(str)

# Aplicar MCA
x_mca = prince.MCA(n_components=15,random_state=42)
x_mca = x_mca.fit(X_cat)

#Transformar dimensiones MCA
X_mca = x_mca.transform(X_cat)

#Varianza explicada acumulada
eigvals = x_mca.eigenvalues_
var_exp = eigvals / eigvals.sum()
cum_var_exp = np.cumsum(var_exp)

# Graficar varianza acumulada
plt.figure(figsize=(8,5))
plt.plot(range(1, len(cum_var_exp)+1), cum_var_exp, marker='o', linestyle='--')
plt.axhline(y=0.85, color='r', linestyle='-')
plt.xlabel('Dimensiones MCA')
plt.ylabel('Varianza acumulada explicada')
plt.title('Varianza acumulada explicada por MCA')
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Coordenadas de las primeras 2 dimensiones
loadings_cat = x_mca.column_coordinates(X_cat).iloc[:, :2]

# Calcular contribución por categoría
loadings_sq = loadings_cat ** 2
contrib_cat = loadings_sq.div(loadings_sq.sum(axis=0), axis=1)

# Extraer nombre de variable original (asumiendo formato 'variable__categoría')
contrib_cat.index = contrib_cat.index.str.split('__').str[0]

# Sumar contribuciones por variable
contrib_var = contrib_cat.groupby(contrib_cat.index).sum().sum(axis=1)

# Normalizar para mostrar como porcentaje
contrib_pct = contrib_var / contrib_var.sum() * 100

# Ordenar
contrib_pct_sorted = contrib_pct.sort_values()

# Graficar horizontalmente
plt.figure(figsize=(12, 10))
sns.barplot(x=contrib_pct_sorted.values, y=contrib_pct_sorted.index, palette='BuGn')
plt.ylabel('Contribución (%) a Dim 1 y 2')
plt.title('Contribución total de todas las variables a las primeras 2 dimensiones MCA')
plt.grid(axis='x', linestyle='--', alpha=0.5)

# Agregar etiquetas a cada barra
for i, v in enumerate(contrib_pct_sorted.values):
    plt.text(v + 0.2, i, f"{v:.2f}%", va='center')

plt.tight_layout()
plt.show()

import numpy as np
X_reduced = np.hstack((X_pca, X_mca.values))

#Seleccionar componentes según varianza acumulada ---
n_pca = np.argmax(explained_var >= 0.85) + 1
X_pca_reduced = X_pca[:, :n_pca]

n_mca = np.argmax(cum_var_exp >= 0.85) + 1
X_mca_reduced = X_mca.iloc[:, :n_mca].values  # convertir a numpy array

#Concatenar
X_reduced = np.hstack((X_pca_reduced, X_mca_reduced))

# Dataframe
pca_col_names = [f"PCA_{i+1}" for i in range(n_pca)]
mca_col_names = [f"MCA_{i+1}" for i in range(n_mca)]
col_names = pca_col_names + mca_col_names

X_reduced_df = pd.DataFrame(X_reduced, columns=col_names)


X_reduced_df.head()